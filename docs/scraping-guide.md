# Gu√≠a de Web Scraping - LivePass (Caf√© Berl√≠n)

## üöÄ Quick Start

### 1. Iniciar el servidor de desarrollo

```bash
npm run dev
```

Espera a que el servidor est√© listo en `http://localhost:3000`

### 2. Ejecutar el scraping

**Opci√≥n A: Con Node.js (Recomendado)**
```bash
node scripts/scrape-livepass.js
```

**Opci√≥n B: Con Bash + curl**
```bash
./scripts/scrape-livepass.sh
```

**Opci√≥n C: Manualmente con curl**
```bash
curl -X POST http://localhost:3000/api/admin/scrape \
  -H "Authorization: Bearer YOUR_ADMIN_API_KEY" \
  -H "Content-Type: application/json"
```

---

## üìä Ejemplo de Output

```
üöÄ Iniciando scraping de LivePass (Caf√© Berl√≠n)...

‚úÖ Scraping completado exitosamente!

üìä Resultados:
   ‚Ä¢ Total eventos scrapeados: 48
   ‚Ä¢ Eventos procesados: 35
   ‚Ä¢ Duplicados detectados: 13
   ‚Ä¢ Errores: 0
   ‚Ä¢ Duraci√≥n: 2847ms

üìã Detalle por fuente:
   ‚úÖ livepass: 48 eventos (2654ms)
```

---

## üîç Verificar los Datos Scrapeados

### Opci√≥n 1: En la UI

1. Ve a `http://localhost:3000/events`
2. Filtra por ciudad: **Buenos Aires**
3. Deber√≠as ver eventos de **Caf√© Berl√≠n**

### Opci√≥n 2: En la base de datos

```bash
# Usando Prisma Studio
npx prisma studio

# O con SQL directo
sqlite3 prisma/dev.db "SELECT title, venue, city, date FROM Event WHERE venue = 'Caf√© Berl√≠n' LIMIT 10;"
```

---

## üõ†Ô∏è Troubleshooting

### Error: "ADMIN_API_KEY no est√° configurado"

Verifica que `.env.local` tenga:
```env
ADMIN_API_KEY="tu-api-key-aqui"
```

> Ver [DEVELOPMENT.md#setup-de-variables-de-entorno](DEVELOPMENT.md#setup-de-variables-de-entorno) para gu√≠a completa de configuraci√≥n.

### Error: "No se puede conectar al servidor"

Aseg√∫rate de que el servidor est√© corriendo:
```bash
npm run dev
```

### Error: "Unauthorized" (401)

Verifica que el `ADMIN_API_KEY` en `.env.local` coincida con el que usas en la request.

### Error: "Failed to scrape livepass"

Posibles causas:
- El sitio LivePass est√° ca√≠do o cambi√≥ su estructura
- Problemas de red
- Rate limiting

Revisa los logs del servidor para m√°s detalles.

---

## üìù Qu√© hace el scraper

El scraper de LivePass (`livepass.config.ts`):

1. **Extrae** eventos de https://livepass.com.ar/taxons/cafe-berlin
2. **Parsea** informaci√≥n:
   - T√≠tulo (limpia " en Caf√© Berl√≠n" del final)
   - Fecha (formato "09 NOV" ‚Üí convierte a Date)
   - Imagen
   - Link al evento
3. **Asigna** valores hardcodeados:
   - Venue: "Caf√© Berl√≠n"
   - City: "Buenos Aires"
   - Country: "AR"
   - Category: "Concierto"
4. **Valida** con `EventBusinessRules`
5. **Deduplica** usando fuzzy matching
6. **Guarda** en la base de datos

---

## üîÑ Automatizaci√≥n (Futuro)

Para ejecutar el scraping autom√°ticamente cada d√≠a:

### Opci√≥n 1: Cron Job (Linux/Mac)
```bash
# Editar crontab
crontab -e

# Agregar l√≠nea (ejecutar a las 2 AM cada d√≠a)
0 2 * * * cd /path/to/envivo && node scripts/scrape-livepass.js >> logs/scraping.log 2>&1
```

### Opci√≥n 2: GitHub Actions
Ver `docs/examples/cicd-example.yml` para workflow de scraping autom√°tico.

### Opci√≥n 3: Vercel Cron Jobs
Ver documentaci√≥n en `docs/PRODUCT.md` (Fase 6: Deployment).

---

## üìñ Referencias

- **Configuraci√≥n del scraper**: `/config/scrapers/livepass.config.ts`
- **Transformaciones custom**: `/src/features/events/data/sources/web/utils/transforms.ts`
  - `parseLivepassDate()`: Parsea "09 NOV" ‚Üí Date
  - `cleanLivepassTitle()`: Remueve " en Caf√© Berl√≠n"
- **Generic Web Scraper**: `/src/features/events/data/sources/web/GenericWebScraper.ts`
- **Orchestrator**: `/src/features/events/data/orchestrator/DataSourceOrchestrator.ts`

---

## üéØ Siguientes Pasos

1. ‚úÖ **Scraper funcionando** (LivePass - Caf√© Berl√≠n)
2. ‚è≥ **Agregar m√°s fuentes**:
   - Passline
   - Otros venues locales (ND Ateneo, Niceto Club, etc.)
3. ‚è≥ **Automatizar** con cron jobs o Vercel Cron
4. ‚è≥ **Monitoring** y alertas si scraping falla

---

**√öltima actualizaci√≥n**: Noviembre 2025
